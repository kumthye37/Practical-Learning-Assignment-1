
Executive Summary

In building this model, we are required to build a machine learning model to predict how well an activity (barbell lifts) was performed using data from the activity monitors.

It appears tbat we have an accuracy of 0.9973 in the final model and accurately predicted how well all 20 activities in the test data set were performed(classe variable).

In order to avoid excessive running time, only codes of the final model would be presented. We can expect the earlier discarded models with higher error rates would only be reported.


Training & Test data set

All data obtained from the following url:

http://groupware.les.inf.puc-rio.br(http://groupware.les.inf.puc-rio.br/har)

The training data:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First, we downloaded files and locate them in the working directory of this project.

The caret and doParallel packages were used in this project.
## Warning: package 'caret' was built under R version 3.1.3
## Loading required package: lattice
## Loading required package: ggplot2
## Warning: package 'ggplot2' was built under R version 3.1.3
## Warning: package 'doParallel' was built under R version 3.1.3
## Loading required package: foreach
## Warning: package 'foreach' was built under R version 3.1.3
## Loading required package: iterators
## Warning: package 'iterators' was built under R version 3.1.3
## Loading required package: parallel


Exploratory data analysis and data cleansing

After loading the training data, we noticed the data set had 19622 rows and 160 columns.
set.seed(50)
Training <- read.csv("pml-training.csv")

dim(Training)
## [1] 19622   160

Columns with low variation, more than 25% null values and descriptive columns (eg. serial number, timestamp) were removed from the data set.
v.0var <- nearZeroVar(Training)

Training <- Training[, -v.0var]

v.length <- sapply(Training, function(x) {
    sum(!(is.na(x) | x == ""))
})


v.null.col <- names(v.length[v.length < 0.75 * length(Training$classe)])

Training <- Training[, !names(Training) %in% v.null.col]

str(Training)
## 'data.frame':    19622 obs. of  59 variables:
##  $ X                   : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ user_name           : Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ raw_timestamp_part_1: int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
##  $ raw_timestamp_part_2: int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...
##  $ cvtd_timestamp      : Factor w/ 20 levels "02/12/2011 13:32",..: 9 9 9 9 9 9 9 9 9 9 ...
##  $ num_window          : int  11 11 11 12 12 12 12 12 12 12 ...
##  $ roll_belt           : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...
##  $ pitch_belt          : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...
##  $ yaw_belt            : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
##  $ total_accel_belt    : int  3 3 3 3 3 3 3 3 3 3 ...
##  $ gyros_belt_x        : num  0 0.02 0 0.02 0.02 0.02 0.02 0.02 0.02 0.03 ...
##  $ gyros_belt_y        : num  0 0 0 0 0.02 0 0 0 0 0 ...
##  $ gyros_belt_z        : num  -0.02 -0.02 -0.02 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02 0 ...
##  $ accel_belt_x        : int  -21 -22 -20 -22 -21 -21 -22 -22 -20 -21 ...
##  $ accel_belt_y        : int  4 4 5 3 2 4 3 4 2 4 ...
##  $ accel_belt_z        : int  22 22 23 21 24 21 21 21 24 22 ...
##  $ magnet_belt_x       : int  -3 -7 -2 -6 -6 0 -4 -2 1 -3 ...
##  $ magnet_belt_y       : int  599 608 600 604 600 603 599 603 602 609 ...
##  $ magnet_belt_z       : int  -313 -311 -305 -310 -302 -312 -311 -313 -312 -308 ...
##  $ roll_arm            : num  -128 -128 -128 -128 -128 -128 -128 -128 -128 -128 ...
##  $ pitch_arm           : num  22.5 22.5 22.5 22.1 22.1 22 21.9 21.8 21.7 21.6 ...
##  $ yaw_arm             : num  -161 -161 -161 -161 -161 -161 -161 -161 -161 -161 ...
##  $ total_accel_arm     : int  34 34 34 34 34 34 34 34 34 34 ...
##  $ gyros_arm_x         : num  0 0.02 0.02 0.02 0 0.02 0 0.02 0.02 0.02 ...
##  $ gyros_arm_y         : num  0 -0.02 -0.02 -0.03 -0.03 -0.03 -0.03 -0.02 -0.03 -0.03 ...
##  $ gyros_arm_z         : num  -0.02 -0.02 -0.02 0.02 0 0 0 0 -0.02 -0.02 ...
##  $ accel_arm_x         : int  -288 -290 -289 -289 -289 -289 -289 -289 -288 -288 ...
##  $ accel_arm_y         : int  109 110 110 111 111 111 111 111 109 110 ...
##  $ accel_arm_z         : int  -123 -125 -126 -123 -123 -122 -125 -124 -122 -124 ...
##  $ magnet_arm_x        : int  -368 -369 -368 -372 -374 -369 -373 -372 -369 -376 ...
##  $ magnet_arm_y        : int  337 337 344 344 337 342 336 338 341 334 ...
##  $ magnet_arm_z        : int  516 513 513 512 506 513 509 510 518 516 ...
##  $ roll_dumbbell       : num  13.1 13.1 12.9 13.4 13.4 ...
##  $ pitch_dumbbell      : num  -70.5 -70.6 -70.3 -70.4 -70.4 ...
##  $ yaw_dumbbell        : num  -84.9 -84.7 -85.1 -84.9 -84.9 ...
##  $ total_accel_dumbbell: int  37 37 37 37 37 37 37 37 37 37 ...
##  $ gyros_dumbbell_x    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ gyros_dumbbell_y    : num  -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 ...
##  $ gyros_dumbbell_z    : num  0 0 0 -0.02 0 0 0 0 0 0 ...
##  $ accel_dumbbell_x    : int  -234 -233 -232 -232 -233 -234 -232 -234 -232 -235 ...
##  $ accel_dumbbell_y    : int  47 47 46 48 48 48 47 46 47 48 ...
##  $ accel_dumbbell_z    : int  -271 -269 -270 -269 -270 -269 -270 -272 -269 -270 ...
##  $ magnet_dumbbell_x   : int  -559 -555 -561 -552 -554 -558 -551 -555 -549 -558 ...
##  $ magnet_dumbbell_y   : int  293 296 298 303 292 294 295 300 292 291 ...
##  $ magnet_dumbbell_z   : num  -65 -64 -63 -60 -68 -66 -70 -74 -65 -69 ...
##  $ roll_forearm        : num  28.4 28.3 28.3 28.1 28 27.9 27.9 27.8 27.7 27.7 ...
##  $ pitch_forearm       : num  -63.9 -63.9 -63.9 -63.9 -63.9 -63.9 -63.9 -63.8 -63.8 -63.8 ...
##  $ yaw_forearm         : num  -153 -153 -152 -152 -152 -152 -152 -152 -152 -152 ...
##  $ total_accel_forearm : int  36 36 36 36 36 36 36 36 36 36 ...
##  $ gyros_forearm_x     : num  0.03 0.02 0.03 0.02 0.02 0.02 0.02 0.02 0.03 0.02 ...
##  $ gyros_forearm_y     : num  0 0 -0.02 -0.02 0 -0.02 0 -0.02 0 0 ...
##  $ gyros_forearm_z     : num  -0.02 -0.02 0 0 -0.02 -0.03 -0.02 0 -0.02 -0.02 ...
##  $ accel_forearm_x     : int  192 192 196 189 189 193 195 193 193 190 ...
##  $ accel_forearm_y     : int  203 203 204 206 206 203 205 205 204 205 ...
##  $ accel_forearm_z     : int  -215 -216 -213 -214 -214 -215 -215 -213 -214 -215 ...
##  $ magnet_forearm_x    : int  -17 -18 -18 -16 -17 -9 -18 -9 -16 -22 ...
##  $ magnet_forearm_y    : num  654 661 658 658 655 660 659 660 653 656 ...
##  $ magnet_forearm_z    : num  476 473 469 469 473 478 470 474 476 473 ...
##  $ classe              : Factor w/ 5 levels "A","B","C","D",..: 1 1 1 1 1 1 1 1 1 1 ...
Training <- Training[, !names(Training) %in% c("X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp")]


dim(Training)
## [1] 19622    55


Building the model with cross-validation

As this was essentially a classification problem, Random Forest function was selected.

We used cross validation to estimate out of sample error and tune the model. Thus, the training data was subset into training and validation set.

In an earlier attempt, 80% of the training data was set for training and 20% for validation. However, the training data was too large for the capacity of the executing machine. Thus, the training data set was reduced to 60% and validation set to 40% for the final model.
# Partition Training set into 40% for validation and 60% for model training
trainset <- createDataPartition(Training$classe, p = 0.6, list = FALSE)
Validation <- Training[-trainset, ]
Training <- Training[trainset, ]

After the above, the training data set was left with 55 columns, including column to be predicted, classe. To further reduce the dimension of the data set, we preprocess the data using PCA with fitting threshold of 0.95, 0.9 and 0.8. All 3 models have 100% in sample accuracy (eg. 0 in sample error) but the expected out of sample accuracy (eg. 1 - out of sample error) were 0.975, 0.9861 and 0.9505 when validated against the validation data set respectively. The results seem to suggest that at PCA threshold of 0.95, there was overfitting and at 0.8, the fitting was too loose. The model with PCA threshold of 0.9 was used to predict how well activities were performed in the test data set. The prediction was as follow (in order of problem id 1 to 20):

B A A A A E D B A A B C B A E E A B B B

We have prediction of A for problem id 3 was incorrect. Thus, we need to further tune the model to get an expected accuracy of higher than 0.9861.


Final Model and final result

PCA is a lossy dimension reduction process and some information could be lost in the process. The model was rebuilt without PCA preprocessing. As expected, this model had 0 in sample error.
cl <- makeCluster(detectCores())
registerDoParallel(cl)

rfModel <- train(classe ~ ., data=Training, method ="rf")
## Loading required package: randomForest
## Warning: package 'randomForest' was built under R version 3.1.3
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
stopCluster(cl)

train.result <- predict(rfModel,Training)

print(table(train.result,Training$classe ))
##             
## train.result    A    B    C    D    E
##            A 3348    0    0    0    0
##            B    0 2279    0    0    0
##            C    0    0 2054    0    0
##            D    0    0    0 1930    0
##            E    0    0    0    0 2165

When validated against the Validation data set, the expected out of sample accuracy was 0.9973
validation.result <- predict(rfModel,Validation[,-55])

print(confusionMatrix(validation.result,Validation$classe))
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2231    9    0    0    0
##          B    1 1508    2    0    0
##          C    0    1 1366    5    0
##          D    0    0    0 1281    3
##          E    0    0    0    0 1439
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9973          
##                  95% CI : (0.9959, 0.9983)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9966          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9996   0.9934   0.9985   0.9961   0.9979
## Specificity            0.9984   0.9995   0.9991   0.9995   1.0000
## Pos Pred Value         0.9960   0.9980   0.9956   0.9977   1.0000
## Neg Pred Value         0.9998   0.9984   0.9997   0.9992   0.9995
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2843   0.1922   0.1741   0.1633   0.1834
## Detection Prevalence   0.2855   0.1926   0.1749   0.1637   0.1834
## Balanced Accuracy      0.9990   0.9965   0.9988   0.9978   0.9990

This model was used to predict how well activities were performed in the test data set again. The prediction was as follow (in order of problem id 1 to 20):
Testing <- read.csv("pml-testing.csv")

Testing <- Testing[,-v.0var]

Testing <- Testing[, !names(Testing) %in% v.null.col]

Testing <- Testing[, !names(Testing) %in% c("X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp")]

test.result <- predict(rfModel,Testing[,-55])

test.result
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E

In the process, id 3 was correctly predicted as B and the rest of the predictions were consistent with earlier correct predictions. Thus, this model was accepted as the final model.
was accepted as the final model.
